{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lesson 20 - Frequency Distributions.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"VuuiZYAMEOFR","colab_type":"text"},"cell_type":"markdown","source":["# 1.0 Frequency Distributions"]},{"metadata":{"id":"spWyvb4nEtJc","colab_type":"text"},"cell_type":"markdown","source":["## 1.1 Simplifying Data"]},{"metadata":{"id":"CrzUD0iwEx6b","colab_type":"text"},"cell_type":"markdown","source":["Previously, we focused on the details around collecting data, on understanding its structure and how it's measured. **Collecting data is just the starting point in a data analysis workflow.** We rarely collect data just for the sake of collecting it. We collect data to analyze it, and we analyze it for different purposes:\n","\n","- To describe phenomena in the world (science).\n","- To make better decisions (industries).\n","- To improve systems (engineering).\n","- To describe different aspects of our society (data journalism); etc.\n","\n","<center><img width=\"250\" src=\"https://drive.google.com/uc?export=view&id=1_QX1g2KHfZVpg5mVsRUO4CJgp0F5StwP\"></center>\n","\n","\n","Our capacity to understand a data set just by looking at it in a table format is limited, and it decreases dramatically as the size of the data set increases. To be able to analyze data, we need to find ways to simplify it.\n","\n","The WNBA data set we've been working with has 143 rows and 32 columns. This might not seem like much compared to other data sets, but it's still extremely difficult to find any patterns just by eyeballing the data set in a table format. With 32 columns, even five rows would take us a couple of minutes to analyze:\n","\n","| _ | Name            | Team | Pos | Height | Weight | BMI       | Birth_Place | Birthdate         | Age | College        | Experience | Games Played | MIN |\n","|---|-----------------|------|-----|--------|--------|-----------|-------------|-------------------|-----|----------------|------------|--------------|-----|\n","| 0 | Aerial Powers   | DAL  | F   | 183    | 71.0   | 21.200991 | US          | January 17, 1994  | 23  | Michigan State | 2          | 8            | 173 |\n","| 1 | Alana Beard     | LA   | G/F | 185    | 73.0   | 21.329438 | US          | May 14, 1982      | 35  | Duke           | 12         | 30           | 947 |\n","| 2 | Alex Bentley    | CON  | G   | 170    | 69.0   | 23.875433 | US          | October 27, 1990  | 26  | Penn State     | 4          | 26           | 617 |\n","| 3 | Alex Montgomery | SAN  | G/F | 185    | 84.0   | 24.543462 | US          | December 11, 1988 | 28  | Georgia Tech   | 6          | 31           | 721 |\n","| 4 | Alexis Jones    | MIN  | G   | 175    | 78.0   | 25.469388 | US          | August 5, 1994    | 23  | Baylor         | R          | 24           | 137 |\n","\n","\n","One way to simplify this data set is to select a variable, count how many times each unique value occurs, and represent the frequencies (the number of times a unique value occurs) in a table. This is how such a table looks for the **POS (player position)** variable:\n","\n","<left><img width=\"150\" src=\"https://drive.google.com/uc?export=view&id=1la98ySTe-ElKFokRsXZTsUyb_RsuB8mL\"></left>\n","\n","\n","Because 60 of the players in our data set play as **guards**, the frequency for guards is 60. Because 33 of the players are forwards, the frequency for forwards is 33, and so on.\n","\n","With the table above, we simplified the **POS variable** by transforming it to a comprehensible format. Instead of having to deal with analyzing 143 values (the length of the POS variable), now we only have five values to analyze. We can make a few conclusions now that would have been difficult and time consuming to reach at just by looking at the list of 143 values:\n","\n","- We can see how the frequencies are distributed:\n","  - Almost half of the players play as guards.\n","  - Most of the players are either guards, forwards or centers.\n","  - Very few players have combined positions (like guard/forward or forward/center).\n","- We can make comparisons with ease:\n","  - There are roughly two times more guards than forwards.\n","  - There are slightly less centers that forwards; etc.\n","  \n","Because the table above shows how frequencies are distributed, it's often called a **frequency distribution table**, or, shorter, **frequency table** or **frequency distribution**. Throughout this mission, our focus will be on learning the details behind this form of simplifying data.\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","Try to get a sense for how difficult it is to analyze the **basketball data set** in its original form.\n","\n","- Read in the basketball data set (the name of the CSV file is **wnba.csv**) using **pd.read_csv()**.\n","- Using **DataFrame.shape**, find the number of rows and columns of the data set.\n","- Print the entire data set, and try to analyze the output to find some patterns."]},{"metadata":{"id":"zC9Pgny3FDFK","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","\n","pd.options.display.max_rows = 200\n","pd.options.display.max_columns = 50\n","\n","# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7JpGjVQ3JuAo","colab_type":"text"},"cell_type":"markdown","source":["## 1.2 Frequency Distribution Tables"]},{"metadata":{"id":"GiYx25X5J5xx","colab_type":"text"},"cell_type":"markdown","source":["A frequency distribution table has two columns. One column records the unique values of a variable, and the other the frequency of each unique value.\n","\n","<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1eZ42ytzaA8E1VUaV_fUti5-kzOEL4Jc5\">\n","\n","To generate a frequency distribution table using Python, we can use the [Series.value_counts()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html?highlight=value_counts#pandas.Series.value_counts) method. Let's try it on the **Pos** column, which describes the position on the court of each individual.\n","\n","```python\n",">> wnba['Pos'].value_counts()\n","G      60\n","F      33\n","C      25\n","G/F    13\n","F/C    12\n","Name: Pos, dtype: int64\n","```\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","Using the **Series.value_counts()** method, generate frequency distribution tables for the following columns:\n","\n","- **Pos.** Assign the frequency distribution table to a variable named **freq_distro_pos**.\n","- **Height.** Assign the frequency distribution table to a variable named **freq_distro_height**.\n","\n","Using the variable inspector, try to analyze each table and identify how values are distributed and compare to each other.\n","\n"]},{"metadata":{"id":"Dflf1YHtRyQY","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mj8uCsF2Tjsq","colab_type":"text"},"cell_type":"markdown","source":["## 1.3 Sorting Frequency Distribution Tables"]},{"metadata":{"id":"8xTh2fibVAXF","colab_type":"text"},"cell_type":"markdown","source":["As you might have noticed, pandas sorts the tables by default in the descending order of the frequencies. Let's consider again the frequency distribution table for the Pos variable, which is measured on a nominal scale:\n","\n","```python\n",">> wnba['Pos'].value_counts()\n","G      60\n","F      33\n","C      25\n","G/F    13\n","F/C    12\n","Name: Pos, dtype: int64\n","```\n","\n","This default is harmless for variables measured on a nominal scale because the unique values, although different, have no direction (we can't say, for instance, that centers are greater or lower than guards). The default actually helps because we can immediately see which values have the greatest or lowest frequencies, we can make comparisons easily, etc.\n","\n","\n","<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1rF3KN5tNdnTS5Jo8veeBk0Q6ANVSH19x\">\n","\n","\n","For variables measured on **ordinal**, **interval**, or **ratio scales**, this default makes the analysis of the tables more difficult because the unique values have direction (some uniques values are greater or lower than others). Let's consider the table for the **Height** variable, which is measured on a **ratio scale**:\n","\n","```python\n",">> wnba['Height'].value_counts()\n","188    20\n","193    18\n","175    16\n","185    15\n","191    11\n","183    11\n","173    11\n","196     9\n","178     8\n","180     7\n","170     6\n","198     5\n","201     2\n","168     2\n","206     1\n","165     1\n","Name: Height, dtype: int64\n"," ```\n"," \n"," Because the **Height** variable has direction, we might be interested to find:\n","\n","- How many players are under 170 cm?\n","- How many players are very tall (over 185)?\n","- Are there any players below 160 cm?\n","\n","It's time-consuming to answer these questions using the table above. The solution is to sort the table ourselves.\n","\n","**wnba['Height'].value_counts()** returns a **Series** object with the measures of height as indices. This allows us to sort the table by index using the [Series.sort_index()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_index.html) method:\n","\n","```python\n",">> wnba['Height'].value_counts().sort_index()\n","165     1\n","168     2\n","170     6\n","173    11\n","175    16\n","178     8\n","180     7\n","183    11\n","185    15\n","188    20\n","191    11\n","193    18\n","196     9\n","198     5\n","201     2\n","206     1\n","Name: Height, dtype: int64\n","```\n","\n","We can also sort the table by index in a descending order using **wnba['Height'].value_counts().sort_index(ascending = False)**\n","\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","- Generate a frequency distribution table for the **Age** variable, which is measured on a **ratio scale**, and sort the table by unique values.\n","  - Sort the table by unique values in an ascending order, and assign the result to a variable named **age_ascending.**\n","  - Sort the table by unique values in a descending order, and assign the result to a variable named **age_descending.**\n","\n","- Analyze one of the frequency distribution tables and brainstorm questions that might be interesting to answer here. These include:\n","  - How many players are under 20?\n","  - How many players are 30 or over?"]},{"metadata":{"id":"9EqKx2muVyXK","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yzuk7gHpZMDk","colab_type":"text"},"cell_type":"markdown","source":["## 1.4 Sorting Tables for Ordinal Variables"]},{"metadata":{"id":"xWwd0C87ZbrY","colab_type":"text"},"cell_type":"markdown","source":["The sorting techniques learned in the previous screen can't be used for ordinal scales where the measurement is done using words. We don't have a variable measured on an ordinal scale in our data set, but let's use the **PTS** variable and the conventions below to create one and see why the techniques we learned don't work:\n","\n","\n","<img width=\"300\" src=\"https://drive.google.com/uc?export=view&id=11c0OU4fvss88CyRX7OBAVetRtgpojL2j\">\n","\n","We name the new column **PTS_ordinal_scale**. Below is a short extract from our data set containing the new column:\n","\n","```python\n",">> wnba[['Name', 'PTS', 'PTS_ordinal_scale']].head()\n","```\n","\n","<img width=\"300\" src=\"https://drive.google.com/uc?export=view&id=1TsbZSmvbqIuXkmfXqk_fuknoWSSpFTiL\">\n","\n","\n","Let's examine the frequency distribution table for the **PTS_ordinal_scale** variable:\n","\n","```python\n",">> wnba['PTS_ordinal_scale'].value_counts()\n","a lot of points    79\n","few points         27\n","many points        25\n","very few points    12\n","dtype: int64\n","```\n","\n","\n","We want to sort the labels in an **ascending** or **descending order**, but using **Series.sort_index()** doesn't work because the method can't infer quantities from words like \"few points\". **Series.sort_index()** can only order the index alphabetically in an ascending or descending order:\n","\n","```python\n",">> wnba['PTS_ordinal_scale'].value_counts().sort_index()\n","a lot of points    79\n","few points         27\n","many points        25\n","very few points    12\n","dtype: int64\n","```\n","\n","The solution is to do selection by index label. The output of **wnba['PTS_ordinal_scale'].value_counts()** is a Series object with the labels as indices. This means we can select by indices to reorder in any way we like:\n","\n","```python\n",">> wnba['PTS_ordinal_scale'].value_counts()[['very few points', 'few points', 'many points', 'a lot of points']]\n","very few points    12\n","few points         27\n","many points        25\n","a lot of points    79\n","dtype: int64\n","```\n","\n","This approach can be time-consuming because it involves more typing than it's ideal. We can use **iloc[]** instead to reorder by position:\n","\n","```python\n",">> wnba['PTS_ordinal_scale'].value_counts().iloc[[3, 1, 2, 0]]\n","very few points    12\n","few points         27\n","many points        25\n","a lot of points    79\n","dtype: int64\n","```\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","We've added more granularity to the ordinal scale above:\n","\n","<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1jYoNYmiO4z0S11WD1Yyj98EnYpZ6ky7S\">\n","\n","\n","Generate a frequency distribution table for the transformed **PTS_ordinal_scale** column.\n","\n","- Order the table by unique values in a descending order (not alphabetically).\n","- Assign the result to a variable named **pts_ordinal_desc.**"]},{"metadata":{"id":"1vPAAgIjb2aI","colab_type":"code","colab":{}},"cell_type":"code","source":["def make_pts_ordinal(row):\n","    if row['PTS'] <= 20:\n","        return 'very few points'\n","    if (20 < row['PTS'] <=  80):\n","        return 'few points'\n","    if (80 < row['PTS'] <=  150):\n","        return 'many, but below average'\n","    if (150 < row['PTS'] <= 300):\n","        return 'average number of points'\n","    if (300 < row['PTS'] <=  450):\n","        return 'more than average'\n","    else:\n","        return 'much more than average'\n","    \n","wnba['PTS_ordinal_scale'] = wnba.apply(make_pts_ordinal, axis = 1)\n","\n","# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fuvqRF3jf7Ye","colab_type":"text"},"cell_type":"markdown","source":["## 1.5 Proportions and Percentages"]},{"metadata":{"id":"ThLnlkemgQAF","colab_type":"text"},"cell_type":"markdown","source":["When we analyze distributions, we're often interested in answering questions about **proportions** and **percentages**. For instance, we may want to answer the following questions about the distribution of the **POS** (player position) variable:\n","\n","- What proportion of players are guards?\n","- What percentage of players are centers?\n","- What percentage of players have mixed positions?\n","\n","It's very difficult to answer these questions precisely just by looking at the frequencies:\n","\n","```python\n",">> wnba['Pos'].value_counts()\n","G      60\n","F      33\n","C      25\n","G/F    13\n","F/C    12\n","Name: Pos, dtype: int64\n","```\n","\n","We can see that almost half of the players are guards, but we need more granularity to answer the first question above. For that, we can transform frequencies to proportions.\n","\n","The proportion of each player position quantifies how many players play in a certain position **relative** to the total number of players. There are 60 guards and 143 players overall (including guards) so the proportion of guards is $\\frac{60}{143}$ .\n","\n","In practical data analysis, it's much more common to express the fraction as a decimal between 0 and 1. So we'd say that $0.42$  (the result of $\\frac{60}{143}$ ) of the players are guards.\n","\n","<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1gXcSf6vRp7hBb_SXeZYlB-_PPeZtyFyH\">\n","\n","In pandas, we can compute all the proportions at once by dividing each frequency to the total number of players:\n","\n","```python\n",">> wnba['Pos'].value_counts() / len(wnba)\n","G      0.419580\n","F      0.230769\n","C      0.174825\n","G/F    0.090909\n","F/C    0.083916\n","Name: Pos, dtype: float64\n","```\n","\n","It's slightly faster though to use **Series.value_counts()** with the **normalize** parameter set to **True**:\n","\n","```python\n",">> wnba['Pos'].value_counts(normalize = True)\n","G      0.419580\n","F      0.230769\n","C      0.174825\n","G/F    0.090909\n","F/C    0.083916\n","Name: Pos, dtype: float64\n","```\n","\n","To find percentages, we just have to multiply the proportions by 100:\n","\n","```python\n",">> wnba['Pos'].value_counts(normalize = True) * 100\n","G      41.958042\n","F      23.076923\n","C      17.482517\n","G/F     9.090909\n","F/C     8.391608\n","Name: Pos, dtype: float64\n","```\n","\n","<img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=1gJzhcwdMVcx2y4DHKHw2LsT_WvyBCz55\">\n","\n","\n","Because proportions and percentages are **relative** to the total number of instances in some set of data, they are called **relative frequencies**. In contrast, the frequencies we've been working with so far are called **absolute frequencies** because they are absolute counts and don't relate to the total number of instances.\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","- Answer the following questions about the Age variable:\n","  - What proportion of players are 25 years old? Assign your answer to a variable named **proportion_25.**\n","  - What percentage of players are 30 years old? Assign your answer to a variable named **percentage_30.**\n","  - What percentage of players are 30 years or older? Assign your answer to a variable named **percentage_over_30**.\n","  - What percentage of players are 23 years or younger? Assign your answer to a variable named **percentage_below_23.**"]},{"metadata":{"id":"K85uB-QXgbUS","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"si2Ocg5gje7R","colab_type":"text"},"cell_type":"markdown","source":["## 1.6 Percentiles and Percentile Ranks"]},{"metadata":{"id":"sr6UKSMgkDFx","colab_type":"text"},"cell_type":"markdown","source":["In the previous exercise, we found that the percentage of players aged 23 years or younger is 19% (rounded to the nearest integer). This percentage is also called a **percentile rank**.\n","\n","A percentile rank of a value $x$  in a frequency distribution is given by the percentage of values that are equal or less than $x$. In our last exercise, $x=23$, and the fact that 23 has a percentile rank of 19% means that 19% of the values are equal to or less than 23.\n","\n","In this context, the value of 23 is called the **19th percentile**. If a value $x$  is the 19th percentile, it means that 19% of all the values in the distribution are equal to or less than $x$ .\n","\n","\n","<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=17b-Z_fRcvHgwPl3NJP5G-B7Pxy-EZ5c1\">\n","\n","\n","When we're trying to answer questions similar to **\"What percentage of players are 23 years or younger?\"**, we're trying to find **percentile ranks.** In our previous exercise, our answer to this question was 18.881%. We can arrive at the same answer a bit faster using the [percentileofscore(a, score, kind='weak')](https://docs.scipy.org/doc/scipy-0.10.0/reference/generated/scipy.stats.percentileofscore.html#scipy-stats-percentileofscore) function from **scipy.stats**:\n","\n","```python\n",">> from scipy.stats import percentileofscore\n",">> percentileofscore(a = wnba['Age'], score = 23, kind = 'weak')\n","18.88111888111888\n","```\n","\n","We need to use **kind = 'weak'** to indicate that we want to find the percentage of values thar are equal to or less than the value we specify in the score parameter.\n","\n","Another question we had was what percentage of players are 30 years or older. We can answer this question too using percentile ranks. First we need to find the percentage of values equal to or less than 29 years (the percentile rank of 29). The rest of the values must be 30 years or more.\n","\n","<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1tLUg6nvpy5nOhXCWVEJgpyvD7ldhjCVw\">\n","\n","\n","In our exercise the answer we found was 26.573%. This is what we get using the technique we've just learned:\n","\n","```python\n",">> 100 - percentileofscore(wnba['Age'], 29, kind = 'weak')\n","26.573426573426573\n","```\n","\n","In the next sections, we'll learn how to find quickly any percentile using pandas. For now, let's practice percentile ranks more.\n","\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","- Import **percentileofscore()** from scipy.stats, and then use it to answer the following questions:\n","  - What percentage of players played half the number of games or less in the 2016-2017 season (there are 34 games in the WNBA’s regular season)? Use the **Games Played** column to find the data you need, and assign your answer to a variable named **percentile_rank_half_less**.\n","  - What percentage of players played more than half the number of games of the season 2016-2017? Assign your result to **percentage_half_more.**"]},{"metadata":{"id":"lPCCRttbn28k","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BYHbwFmr3au_","colab_type":"text"},"cell_type":"markdown","source":["## 1.7 Finding Percentiles with pandas"]},{"metadata":{"id":"IatkMlRs3nvs","colab_type":"text"},"cell_type":"markdown","source":["To find percentiles, we can use the **Series.describe()** method, which returns by default the **25th**, the **50th**, and the **75th** percentiles:\n","\n","```python\n",">> wnba['Age'].describe()\n","count    143.000000\n","mean      27.076923\n","std        3.679170\n","min       21.000000\n","25%       24.000000\n","50%       27.000000\n","75%       30.000000\n","max       36.000000\n","Name: Age, dtype: float64\n","```\n","\n","We are not interested here in the first three rows of the output (count, mean, and standard deviation). We can use **iloc[]** to isolate just the output we want:\n","\n","```python\n",">>wnba['Age'].describe().iloc[3:]\n","min    21.0\n","25%    24.0\n","50%    27.0\n","75%    30.0\n","max    36.0\n","Name: Age, dtype: float64\n","```\n","\n","The **25th**, **50th**, and **75th** percentiles pandas returns by default are the scores that divide the distribution into four equal parts.\n","\n","<img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1UI-X7sTvPKAa4xkygiBM8cxkxjpeCBlL\">\n","\n","\n","The three percentiles that divide the distribution in four equal parts are also known as **quartiles** (from the Latin [quartus](http://www.latin-dictionary.net/definition/32600/quattuor-quartus) which means four). There are three quartiles in the distribution of the **Age** variable:\n","\n","- The first quartile (also called lower quartile) is 24 (note that 24 is also the 25th percentile).\n","- The second quartile (also called the middle quartile) is 27 (note that 27 is also the 50th percentile).\n","- And the third quartile (also called the upper quartile) is 30 (note that 30 is also the 75th percentile).\n","\n","We may be interested to find the percentiles for percentages other than 25%, 50%, or 75%. For that, we can use the **percentiles** parameter of **Series.describe()**. This parameter requires us to pass the **percentages** we want as proportions between 0 and 1.\n","\n","\n","```python\n",">> wnba['Age'].describe(percentiles = [.1, .15, .33, .5, .592, .85, .9]).iloc[3:]\n","min      21.0\n","10%      23.0\n","15%      23.0\n","33%      25.0\n","50%      27.0\n","59.2%    28.0\n","85%      31.0\n","90%      32.0\n","max      36.0\n","Name: Age, dtype: float64\n","```\n","\n","Percentiles don't have a single [standard definition](https://en.wikipedia.org/wiki/Percentile#Definitions), so don't be surprised if you get very similar (but not identical) values if you use different functions (especially if the functions come from different libraries).\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","- Use the **Age** variable along with **Series.describe()** to answer the following questions:\n","  - What's the upper quartile of the **Age** variable? Assign your answer to a variable named **age_upper_quartile.**\n","  - What's the middle quartile of the **Age** variable? Assign your answer to a variable named **age_middle_quartile.**\n","  - What's the **95th percentile** of the **Age** variable? Assign your answer to a variable named **age_95th_percentile.**\n","- Indicate the truth value of the following sentences:\n","  - A **percentile** is a value of a variable, and it corresponds to a certain **percentile rank** in the distribution of that variable. (If you think this is true, assign **True** (boolean, not string) to a variable named **question_1**, otherwise assign **False**.)\n","  - A **percentile rank** is a numerical value from the distribution of a variable. (Assign **True** or **False** to **question_2**.)\n","  - The **25th percentile** is the same thing as the **lower quartile**, and the **upper quartile** is the same thing as the **third quartile**. (Assign **True** or **False** to **question_3**)\n","\n"]},{"metadata":{"id":"ioZotk1Q3wPn","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W5ai_b-zBbWM","colab_type":"text"},"cell_type":"markdown","source":["## 2.8 Grouped Frequency Distribution Tables"]},{"metadata":{"id":"U4ctTbziBowH","colab_type":"text"},"cell_type":"markdown","source":["With frequency tables, we're trying to transform relatively large and incomprehensible amounts of data to a table format we can understand. However, not all frequency tables are straightforward:\n","\n","\n","```python\n",">> wnba['Weight'].value_counts().sort_index()\n","55.0      1\n","57.0      1\n","58.0      1\n","59.0      2\n","62.0      1\n","63.0      3\n","64.0      5\n","65.0      4\n","66.0      8\n","67.0      1\n","68.0      2\n","69.0      2\n","70.0      3\n","71.0      2\n","73.0      6\n","74.0      4\n","75.0      4\n","76.0      4\n","77.0     10\n","78.0      5\n","79.0      6\n","80.0      3\n","81.0      5\n","82.0      4\n","83.0      4\n","84.0      9\n","85.0      2\n","86.0      7\n","87.0      6\n","88.0      6\n","89.0      3\n","90.0      2\n","91.0      3\n","93.0      3\n","95.0      2\n","96.0      2\n","97.0      1\n","104.0     2\n","108.0     1\n","113.0     2\n","Name: Weight, dtype: int64\n","```\n","\n","\n","There's a lot of granularity in the table above, but for this reason it's not easy to find patterns. The table for the **Weight** variable is a relatively happy case - the frequency tables for variables like **PTS**, **BMI**, or **MIN** are even more **daunting**.\n","\n","If the variable is measured on an **interval** or **ratio scale**, a common solution to this problem is to **group the values in equal intervals**. For the **Weight** variable, the values range from 55 to 113 kg, which amounts to a difference of 58 kg. We can try to segment this 58 kg interval in ten smaller and equal intervals. This will result in ten intervals of 5.8 kg each:\n","\n","<img width=\"450\" src=\"https://drive.google.com/uc?export=view&id=12ar9ft_-oVZpZWH_hcjz3awBmhB-ADHg\">\n","\n","\n","Fortunately, pandas can handle this process gracefully. We only need to make use of the **bins** parameter of **Series.value_counts()**. We want ten equal intervals, so we need to specify **bins = 10**:\n","\n","\n","```python\n",">> wnba['Weight'].value_counts(bins = 10).sort_index()\n","(54.941, 60.8]     5\n","(60.8, 66.6]      21\n","(66.6, 72.4]      10\n","(72.4, 78.2]      33\n","(78.2, 84.0]      31\n","(84.0, 89.8]      24\n","(89.8, 95.6]      10\n","(95.6, 101.4]      3\n","(101.4, 107.2]     2\n","(107.2, 113.0]     3\n","Name: Weight, dtype: int64\n"," ```\n"," \n","(54.941, 60.8], (60.8, 66.6] or (107.2, 113.0] are number intervals. The ( character indicates that the starting point is not included, while the ] indicates that the endpoint is included. (54.941, 60.8] means that 54.941 isn't included in the interval, while 60.8 is. The interval (54.941, 60.8] contains all real numbers greater than 54.941 and less than or equal to 60.8.\n","\n","We can see above that there are 10 equal intervals, 5.8 each. The first interval, (54.941, 60.8] is confusing, and has to do with [how pandas internals show the output](https://github.com/pandas-dev/pandas/blob/01e99decf14b55409cea0789ffcc615afed45bac/pandas/core/algorithms.py#L497). One way to understand this is to convert 54.941 to 1 decimal point, like all the other values are. Then the first interval becomes (54.9, 60.8]. 54.9 is not included, so you can think that the interval starts at the minimum value of the Weight variable, which is 55.\n","\n","Because we group values in a table to get a better sense of frequencies in the distribution, the table we generated above is also known as a **grouped frequency distribution table**. Each group (interval) in a grouped frequency distribution table is also known as a **class interval**. (107.2, 113.0], for instance, is a class interval.\n","\n","Using the grouped frequency distribution table we generated above for the **Weight** variable, we can find patterns easier in the distribution of values:\n","\n","- Most players weigh somewhere between 70 and 90 kg.\n","- Very few players weigh over 100 kg.\n","- Very few players weigh under 60 kg; etc.\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","Verify on the cell below and examine the frequency table for the **PTS** (total points) variable trying to find some patterns in the distribution of values. Then, generate a grouped frequency distribution table for the **PTS** variable with the following characteristics:\n","- The table has 10 class intervals.\n","- For each class interval, the table shows percentages instead of frequencies.\n","- The class intervals are sorted in descending order.\n","- Assign the table to a variable named **grouped_freq_table**, then print it and try again to find some patterns in the distribution of values.\n"]},{"metadata":{"id":"DGJwq4tdBz3H","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tNSbzczYEI-A","colab_type":"text"},"cell_type":"markdown","source":["## 1.9 Information Loss"]},{"metadata":{"id":"9_wnh6DxEP9g","colab_type":"text"},"cell_type":"markdown","source":["When we generate grouped frequency distribution tables, there's an inevitable information loss. Let's consider this table:\n","\n","```python\n",">> wnba['PTS'].value_counts(bins = 10)\n","(1.417, 60.2]     30\n","(60.2, 118.4]     24\n","(118.4, 176.6]    17\n","(176.6, 234.8]    20\n","(234.8, 293.0]    17\n","(293.0, 351.2]     8\n","(351.2, 409.4]    10\n","(409.4, 467.6]     8\n","(467.6, 525.8]     4\n","(525.8, 584.0]     5\n","Name: PTS, dtype: int64\n"," ```\n"," \n","Looking at the first interval, we can see there are 30 players who scored between 2 and 60 points (2 is the minimum value in our data set, and points in basketball can only be integers). However, because we grouped the values, we lost more granular information like:\n","\n","- How many players, if any, scored exactly 50 points.\n","- How many players scored under 10 points.\n","- How many players scored between 20 and 30 points, etc.\n","\n","To get back this granular information, we can increase the number of class intervals. However, if we do that, we end up again with a table that's lengthy and very difficult to analyze.\n","\n","On the other side, if we decrease the number of class intervals, we lose even more information:\n","\n","```python\n","wnba['PTS'].value_counts(bins = 5).sort_index()\n","(1.417, 118.4]    54\n","(118.4, 234.8]    37\n","(234.8, 351.2]    25\n","(351.2, 467.6]    18\n","(467.6, 584.0]     9\n","Name: PTS, dtype: int64\n","```\n","\n","There are 54 players that scored between 2 and 118 points. We can get this information from the first table above too, but there's some extra information there: among these 54 players, 30 scored between 2 and 60 points, and 24 scored between 61 and 118 points. We lost this information when we decreased the number of class intervals from 10 to 5.\n","\n","We can conclude there is a trade-off between the information in a table, and how comprehensible the table is.\n","\n","\n","<img width=\"450\" src=\"https://drive.google.com/uc?export=view&id=1PtWCiK-ocbotjHqhFxmWL-3j_kAdP9au\">\n","\n","\n","\n","When we increase the number of class intervals, we can get more information, but the table becomes harder to analyze. When we decrease the number of class intervals, we get a boost in comprehensibility, but the amount of information in the table decreases.\n","\n","As a rule of thumb, 10 is a good number of class intervals to choose because it offers a good balance between information and comprehensibility.\n","\n","<img width=\"450\" src=\"https://drive.google.com/uc?export=view&id=1MaCcxrDyJhXd6smlAt-eK41rqRImqKyi\">\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","Generate a grouped frequency distribution for the **MIN** variable (minutes played during the season), and experiment with the number of class intervals to get a sense for what conclusions you can draw as you vary the number of class intervals. Try to experiment with the following numbers of class intervals:\n","- 1\n","- 2\n","- 3\n","- 5\n","- 10\n","- 15\n","- 20\n","- 40"]},{"metadata":{"id":"aalHcCmoGYuP","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pUzyuKd3EUAC","colab_type":"text"},"cell_type":"markdown","source":["## 1.10 Readability for Grouped Frequency Tables"]},{"metadata":{"id":"QgCqGq7EGN7L","colab_type":"text"},"cell_type":"markdown","source":["Pandas helps a lot when we need to explore quickly grouped frequency tables. However, the intervals pandas outputs are confusing at a first sight:\n","\n","```python\n","wnba['PTS'].value_counts(bins = 5).sort_index()\n","(1.417, 118.4]    54\n","(118.4, 234.8]    37\n","(234.8, 351.2]    25\n","(351.2, 467.6]    18\n","(467.6, 584.0]     9\n","Name: PTS, dtype: int64\n","```\n","\n","Imagine we'd have to publish the table above in a blog post or a scientific paper. The readers will have a hard time understanding the intervals we chose. They'll also be puzzled by the decimal numbers because points in basketball can only be integers.\n","\n","To fix this, we can define the intervals ourselves. For the table above, we can define six intervals of 100 points each, and then count how many values fit in each interval. We'd like to end with a table like this:\n","\n","\n","```python\n","(0,100]      49\n","(100,200]    28\n","(200,300]    32\n","(300,400]    17\n","(400,500]    10\n","(500,600]     7\n","```\n","\n","Next, we show one way to code the intervals. We start with creating the intervals using the [pd.interval_range()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.interval_range.html?highlight=interval_rang) function:\n","\n","```python\n",">> intervals = pd.interval_range(start = 0, end = 600, freq = 100)\n",">> intervals\n","IntervalIndex([(0, 100], (100, 200], (200, 300], (300, 400], (400, 500], (500, 600]]\n","              closed='right',\n","              dtype='interval[int64]')\n","```\n","\n","              \n","Next, we create a new Series using the intervals as indices, and, for now, 0 as values:\n","\n","```python\n",">> gr_freq_table = pd.Series([0,0,0,0,0,0], index = intervals)\n",">> gr_freq_table\n","(0, 100]      0\n","(100, 200]    0\n","(200, 300]    0\n","(300, 400]    0\n","(400, 500]    0\n","(500, 600]    0\n","dtype: int64\n","```\n","\n","Next, we loop through the values of the **PTS** column, and for each value:\n","\n","- We loop through the intervals we defined previously, and for each interval:\n","  - We check whether the current value from the PTS column belongs to that interval.\n","  - If the value doesn't belong to an interval, we continue the inner loop over the intervals.\n","  - If the value belongs to an interval:\n","    - We update the counting for that interval in **gr_freq_table** by adding 1.\n","    - We exit the inner loop over the intervals with **break** because a value can belong to one interval only, and it makes no sense to continue the loop (without using **break**, we'll get the same output but we'll do many redundant iterations).\n","    \n","    \n","```python\n",">> for value in wnba['PTS']:\n","       for interval in intervals:\n","           if value in interval:\n","               gr_freq_table.loc[interval] += 1\n","               break\n",">> gr_freq_table\n","(0, 100]      49\n","(100, 200]    28\n","(200, 300]    32\n","(300, 400]    17\n","(400, 500]    10\n","(500, 600]     7\n","dtype: int64\n","```\n","\n","\n","Now we do a quick sanity check of our work. There are 143 players in the data set, so the frequencies should add up to 143:\n","\n","```python\n",">> gr_freq_table.sum()\n","143\n","```\n","\n","Note that we're not restricted by the minimum and maximum values of a variable when we define intervals. The minimum number of points is 2, and the maximum is 584, but our intervals range from 1 to 600.\n","\n","\n","**Exercise**\n","\n","<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n","\n","\n","- Using the techniques above, generate a grouped frequency table for the **PTS** variable. The table should have the following characteristics:\n","  - There are 10 class intervals.\n","  - The first class interval starts at 0 (not included).\n","  - The last class interval ends at 600 (included).\n","  - Each interval has a range of 60 points.\n","- Assign the table to a variable named **gr_freq_table_10**."]},{"metadata":{"id":"aNCIWc8gG0eo","colab_type":"code","colab":{}},"cell_type":"code","source":["# put your code here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uo0Qb2l_IZ75","colab_type":"text"},"cell_type":"markdown","source":["## 1.11 Frequency Tables and Continuous Variables\n"]},{"metadata":{"id":"DOLArazU4k1H","colab_type":"text"},"cell_type":"markdown","source":["Remember from the previous mission that a height of 175 cm is just an interval bounded by the real limits of 174.5 cm (lower real limit) and 175.5 (upper real limit). When we build frequency tables for continuous variables, we need to take into account that the values are intervals.\n","\n","The height of 175 cm has a frequency of 16 in the distribution of the **Height** variable:\n","\n","```python\n",">> wnba['Height'].value_counts()[175]\n","16\n","```\n","\n","This doesn't mean that there are 16 players that are all exactly 175 cm tall. It rather means that there are 16 players with a height that's somewhere between 174.5 cm and 175.5 cm.\n","\n","A similar reasoning applies when we read grouped frequency tables. If we had an interval of (180, 190] for a continuous variable, 180 and 190 are not the real limits. Instead, the real limits are given by the interval (179.5, 190.5], with 179.5 being the lower real limit of 180, and 190.5 the upper real limit of 190.\n","\n","Continuous variables affect as well the way we read percentiles. For instance, the 50th percentile (middle quartile) in the distribution of the **Height** variable is 185 cm:\n","\n","\n","```python\n",">> wnba['Height'].describe().iloc[3:]\n","min    165.0\n","25%    176.5\n","50%    185.0\n","75%    191.0\n","max    206.0\n","Name: Height, dtype: float64\n","```\n","\n","This means that 50% of the values are less or equal to 185.5 cm (the upper limit of 185 cm), not to 185 cm.\n","\n","\n"]},{"metadata":{"id":"fQaUp50e47Hr","colab_type":"text"},"cell_type":"markdown","source":["## 1.12 Next Steps\n","\n"]},{"metadata":{"id":"SlMNgBBT55da","colab_type":"text"},"cell_type":"markdown","source":["In this mission, we learned how to organize data in frequency and grouped frequency tables. Frequency tables allow us to transform large and incomprehensible amounts of data to a format we can understand.\n","\n","\n","\n","<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1Zp0BuFcG3T0rk4MSaPfw8gpGPdlrQ68I\">\n","\n","Next in the course, we'll learn how to visualize frequency tables using bar plots and histograms.\n","\n"]}]}